{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this phase, we treat everything we identified in the EDA phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data preprocessing and utilities\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Machine learning models and tools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../database/cleaned_dataset.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There were no missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Encoding\n",
    "\n",
    "As mentioned before, we use the **One-Hot Encoding** for features with less than 10 classes, while we use the **Label Encoding** for the others that have a higher count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Features and Target\n",
    "X = df.drop([\"TARGET\"], axis=1)\n",
    "y = df[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Strategy for Features\n",
    "# Split categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns # This composed of only the \"residence_status\"\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot Encoding on the categorical columns (which is only the residence_status)\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split, SMOTE, and Scaling\n",
    "\n",
    "Here we perform the *train_test_split* in a custom way.\n",
    "\n",
    "In fact, in this phase, we:\n",
    "1. Split the dataset in train and test (using 80% / 20% division), considering the **stratify** parameter to leave the same distributions of the target variable.\n",
    "2. Remove outliers.\n",
    "3. Apply SMOTE to the minority class to make them more balanced.\n",
    "4. Scale the features using a **Standard Scaler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sampling_strategy(y, multiplier=10):\n",
    "    \"\"\"\n",
    "    Calculate the sampling strategy for SMOTE for a binary classification problem.\n",
    "\n",
    "    Parameters:\n",
    "        y (array-like): Target variable for the training set.\n",
    "        multiplier (int): Factor by which the minority class will be oversampled.\n",
    "\n",
    "    Returns:\n",
    "        dict: Sampling strategy for SMOTE.\n",
    "    \"\"\"\n",
    "    unique_classes = pd.Series(y).value_counts()\n",
    "    minority_class = unique_classes.idxmin()  # Identify the minority class (class with fewer samples)\n",
    "    current_count = unique_classes[minority_class]\n",
    "    \n",
    "    # Define the sampling strategy for the minority class\n",
    "    sampling_strategy = {minority_class: current_count * multiplier}\n",
    "    return sampling_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(X, y, sampling_strategy, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to balance the minority class in the training set.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Training features.\n",
    "        y (array-like): Training labels.\n",
    "        sampling_strategy (dict): Sampling strategy for SMOTE.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Resampled training features and labels.\n",
    "    \"\"\"\n",
    "    if sampling_strategy:\n",
    "        smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
    "        return smote.fit_resample(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test, numerical_cols):\n",
    "    \"\"\"\n",
    "    Scale numerical features using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training feature set.\n",
    "        X_test (pd.DataFrame): Test feature set.\n",
    "        numerical_cols (list): List of numerical feature columns.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Scaled training and test feature sets.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "    X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(\n",
    "    X, y, test_size=0.2, multiplier=10, random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a stratified train-test split, apply SMOTE to balance the minority class, and scale features.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature dataset.\n",
    "        y (pd.Series): Target variable.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        multiplier (int): Factor by which the minority class will be oversampled.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Training and test sets with resampled training labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stratified train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Calculate SMOTE sampling strategy\n",
    "    sampling_strategy = calculate_sampling_strategy(y_train, multiplier)\n",
    "\n",
    "    # Apply SMOTE to balance the training set\n",
    "    X_train_resampled, y_train_resampled = apply_smote(X_train, y_train, sampling_strategy, random_state)\n",
    "\n",
    "    # Scale numerical features\n",
    "    numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    X_train_scaled, X_test_scaled = scale_features(X_train_resampled, X_test, numerical_cols)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_resampled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = custom_train_test_split(\n",
    "    X, y, test_size=0.2, multiplier=10, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (59993, 21), y_train: 59993\n",
      "X_test: (13575, 21), y_test: 13575\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes\n",
    "print(f\"X_train: {X_train.shape}, y_train: {len(y_train)}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Class in y_train: 53663\n",
      "0 Class in y_test: 13417\n",
      "\n",
      "1 Class in y_train: 6330\n",
      "1 Class in y_test: 158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through all unique classes in y_train and y_test\n",
    "for cls in set(y_train).union(set(y_test)):\n",
    "    train_count = len(y_train[y_train == cls])\n",
    "    test_count = len(y_test[y_test == cls])\n",
    "    print(f\"{cls} Class in y_train: {train_count}\")\n",
    "    print(f\"{cls} Class in y_test: {test_count}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing datasets saved in 'database/preprocessed_dataset/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = \"../database/preprocessed_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save training data\n",
    "X_train.to_csv(f\"{output_dir}/X_train.csv\", index=False)\n",
    "y_train.to_csv(f\"{output_dir}/y_train.csv\", index=False)\n",
    "\n",
    "# Save testing data\n",
    "X_test.to_csv(f\"{output_dir}/X_test.csv\", index=False)\n",
    "y_test.to_csv(f\"{output_dir}/y_test.csv\", index=False)\n",
    "\n",
    "print(\"Training and testing datasets saved in 'database/preprocessed_dataset/'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
